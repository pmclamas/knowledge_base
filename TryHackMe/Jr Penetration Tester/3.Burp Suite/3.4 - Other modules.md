We will be looking at the Decoder, Comparer and Sequencer tools. These allows us to: work with encoded text; compare sets of text; and analyse the randomness of captured tokens, respectively. Being able to perform these relatively straighforward tasks directly within Burp Suite can save a lot of time.

**Decoder**
The Burp Suite Decoder module allows us to manipulate data. As the name suggests, we can decode information that we capture during an attack, but we can also encode data of our own, ready to be sent to the target. Decoder also allows us to create hashsums of data, as well as providing a Smart Decode feature which attempts to decode provided data recursively until it is back to being plaintext.

The Decoder interface offers us a number of options:
1. The box on the left is where we would paste or type text to be encoded or decoded. As with most other modules of Burp Suite, we can also send data here from other sections of the framework by right-clicking and choosing Send to Decoder.
2. We have the option to select between treating the input as text or hexadecimal byte values at the top of the list on the right.
3. Further down the list, we have dropdown menus to Encode, Decode or Hash the input.
4. Finally, we have the "Smart Decode" feature, which attempts to decode the input automatically.

**Decoder: Encoding/Decoding**
Let's take a closer look at the manual encoding and decoding options. These are the same whether we choose the decoding or encoding menu:
- _Plain:_ plaintext is what we have before perfoming any transformations.
- _URL:_ URL encoding is used to make data safe to transfer in the URL of a web request. It involves exchanging characters for their ASCII character code in hexadecimal format, preceded by a percentage symbol. URL encoding is an extremely useful method to know for any kind of web application testing.
- _HTML:_ encoding text as html Entities involves replacing special characters with an & followed by either a hexadecimal number or a reference to the character being escaped, then a semicolon. This encoding method allows special characters in the html language to be rendered safely in html pages and has the added bonus of being used to prevent attacks such as XSS (Cross-Site Scripting).
  When we use the html option in Decoder, we can encode any character as its html escaped format or decode captured html entities.
  - _Base64:_ another widely used encoding method, base64 is used to encode any data in an ASCII-compatible format. It was designed to take binary data (e.g. images, media, programs) and encode it in a format that would be suitable to transfer over virtually any medium.
  - _ASCII Hex:_ this option coverts data between ASCII representation and hexadecimal representation. Each letter in the original data is taken individually and converted from numeric ASCII representation into hexadecimal.
  - _Hex, Octal, and Binary:_ these encoding methods all apply only to numeric inputs. They convert between decimal, hexadecimal, octal (base eight) and binary.
  - _Gzip:_ provides a way to compress data. It is widely used to reduce the size of files and pages before they are sent to your browser.

**Hex Format:**
Inputting data in ASCII format is great, but sometimes we need to be able to edit our input byte-by-byte. For this, we can use "Hex view" by choosing "Hex" above the decoding options.
This setting allows us to view and edit our text in hexadecimal byte format -- a very useful trick if we are working with binary files or other non-ASCII data.

**Smart Decode:**
This feature of Decoder attempts to automatically decode encoded text.
This feature is far form perfect, but it can be very useful for quickly decoding chunks of unknown data.

----------------------------------

**Decoder: Hashing**
Hashing is a one-way process that is used to transform data into a unique signature. To be a hashing algorithm, the resulting output must be impossible to reverse. A good hashing algorithm will ensure that every piece of data entered will have a completely unique hash.
Hashes are frequently used to verify the integrity of files and documents, as even a very small change to the file will result in the hashsum changing significantly.

Note: the MD5 algorithm is deprecated and should not be used for modern applications.

Equally, hashes are also used to securely store passwords as (due to the one-way hashing process meaning that the hashes can never be reversed) the passwords will be (relatively) secure if the database is leaked. 
When a user creates a password, it is hashed and stored by the application. When the user tries to log in, the application will then hash the password they submit and check it against the stored hash; if the hashes match, then the password was correct. When using this methodology, an application never has to store the original (plaintext) password.

---------------------------
**Comparer - Overview**
Comparer allows us to compare two piece of data, either by ASCII words or by bytes.
The Comparer interface can be split into three main parts:
1. On the left, we have the items being compared. When we load data into Comparer, it will appear as rows in thee tables -- we would then select two datasets to compare.
2. On the upper right, we have options for pasting data in from the clipboard (Paste), loading data from a file (Load), removing the current row (Remove) and clearing all datasets (Clear).
3. Finally, on the bottom right, we have the option to compare our datasets by either words or bytes. Don't worry about which of these buttons you select as this can be changed later on. These are the buttons we click when we are ready to compare the data we have selected.

We can also load data into Comparer from other modules by right-clicking and choosing "Send to Comparer".

**_Conclusion:_**
There are many situations where being able to quickly compare two (potentially very large) pieces of data can come in handy.
For example, when performing a login bruteforce or credential stuffing attack with intruder, you may wish to compare two responses with different lengths to see where the differences lie and whether the differences indicate a successful login.

--------------------------------
**Sequencer - Overview**
Sequencer is one of those tools that rarely ever gets used in CTFs and other lab environments, but is an essential part of a real-world web app penetration test.
In short, Sequencer allows us to measure the randomness of "tokens" -- strings that are used to identify somethind and should, in theory, be generated in a cryptographically secure manner. For example, we may wish to analyse the randomness of a session cookie or a Cross-Site Request Forgery (CSRF) token protecting a form submission. If it turns out that these tokens are not generated securely, then we can (in theory) predict the values of upcoming tokens. 

There are two main methods we can use to perform token analysis with Sequencer:
- live capture is the more common of the two methods -- this is the default sub-tab for Sequencer. Live capture allows us to pass a request to Sequencer, which we know will create a token for us to analyse. For example, we may wish to pass a POST request to a login endpoint into Sequencer, as we know that the server will respond by giving us a cookie. With the request passed in, we can tell Sequencer to start a live capture: it will then make the same request thousands of times automatically, storing the generated token samples for analysis. Once we have accumulated enough samples, we stop Sequencer and allow it to analyse the captured tokens.
- Manual load allows us to load a list of pre-generated token samples straight into Sequencer for analysis. Using Manual Load means we don't have to make thousands of requests to our target (which is both loud and resource intensive), but it does mean that we need to obtain a large list of pre-generated tokens!

**Sequencer - Analysis**
Burp performs dozens of tests on the token samples that it captured. 

The generated entropy analysis report is split into four primary sections -- the first of these being a summary of the results:
![[Screenshot 2022-03-26 at 15.00.52.png]]
The summary gives us an overall result; the effective entropy; an analysis of the reliability of the results; and a summary of the sample taken.

Collectively, these will often be enough to determine whether the token is generated safely or not; however, in some instances, we may need to have a look at the test results directly -- this can be done in the "Character-level analysis" an "Bit-level analysis" tabs.

